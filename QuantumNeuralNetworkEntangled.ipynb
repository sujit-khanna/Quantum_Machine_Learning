{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# Implementation of a pure (non-hybrid) Quantum Neural Network with entangled states\n",
    "## The notebook below implements a multi-layered perceptron based Neural Network with hidden layers implemented replicated by varational circuits\n",
    "    - The implementation is a muticlass classifier that tries to predict the 3 output classes\n",
    "    - This is a novel algorithm that is not implemented on the pennylane tutorial website\n",
    "    - The dataset used is IRIS flower classification dataset, with 5 input features and 3 output classes\n",
    "    - The performance is evaluated on the test dataset with train-test split of 75%:25%import pennylane as qml\n",
    "from pennylane import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "from pennylane.optimize import NesterovMomentumOptimizer, AdamOptimizer\n",
    "from pennylane.init import strong_ent_layers_uniform\n",
    "from pennylane.templates.layers import StronglyEntanglingLayers\n",
    "from pennylane.templates import AmplitudeEmbedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_qubits = 3 # for representing three output classes\n",
    "num_layers = 3 # number of hidden layers of the quantum neural network\n",
    "q_depth = num_layers # Circuit depth which is also equal to num_layers\n",
    "l_r = 0.1 # Learning rate for the optimizer\n",
    "num_epochs = 100 # number of training epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiating our quantum simulator at the backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = qml.device(\"default.qubit\", wires=n_qubits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the hidden quantum layer which will be repeated num_layers times i.e. 6 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "@qml.qnode(dev)\n",
    "def quantum_net(weights, x=None):\n",
    "    \"\"\"\n",
    "    The variational quantum circuit.\n",
    "    \"\"\"\n",
    "    AmplitudeEmbedding(x, wires=range(n_qubits),normalize=True, pad=0.3)\n",
    "    # AngleEmbedding(x, wires=range(n_qubits))\n",
    "    StronglyEntanglingLayers(weights, wires=range(n_qubits))\n",
    "    return tuple(qml.expval(qml.PauliZ(wires=i)) for i in range(n_qubits))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representation of a single layered QNN"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "One layer QNN Classifier:\n",
    "\n",
    " 0: ──╭QubitStateVector(M0)──Rot(2.353, 5.974, 4.599)──╭C──────╭X──┤ ⟨Z⟩ \n",
    " 1: ──├QubitStateVector(M0)──Rot(3.761, 0.98, 0.98)────╰X──╭C──│───┤ ⟨Z⟩ \n",
    " 2: ──╰QubitStateVector(M0)──Rot(0.365, 5.442, 3.777)──────╰X──╰C──┤ ⟨Z⟩ \n",
    "\n",
    "M0=Input features\n",
    "M0 =\n",
    "[0.80642366 0.5315065  0.25658935 0.03665562 0.         0.\n",
    " 0.         0.        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the cost function of the classifier i.e. categorical crossentropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X):\n",
    "    prob = np.exp(X)\n",
    "    probs = prob / np.sum(prob, axis=1, keepdims=True)\n",
    "    return probs\n",
    "def cost(thetas, X, actual_labels):\n",
    "    b = X.shape[0]\n",
    "    yhats = []\n",
    "    for i in range(b):\n",
    "        yhat = quantum_net(thetas[0], x=X[i])\n",
    "        # print(quantum_net.draw())\n",
    "        yhats.append(yhat)\n",
    "    st = np.stack(yhats)\n",
    "    st_prob = softmax(st)\n",
    "    y = np.zeros((actual_labels.shape[0], n_qubits))\n",
    "    y[np.arange(actual_labels.shape[0]), actual_labels] = 1\n",
    "    return -np.mean(y*np.log(st_prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the training procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First X sample (padded)    : [5.1 3.5 1.4 0.2]\n"
     ]
    }
   ],
   "source": [
    "num_features = 4\n",
    "data = np.loadtxt(\"multiclass_classification/iris.csv\", delimiter=\",\")\n",
    "X = data[:, 0:num_features]\n",
    "X_pad = X\n",
    "print(\"First X sample (padded)    :\", X_pad[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Normalizing the dataset and creating training and validation(test) set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First X sample (normalized): [0.80377277 0.55160877 0.22064351 0.0315205 ]\n"
     ]
    }
   ],
   "source": [
    "# normalize each input\n",
    "normalization = np.sqrt(np.sum(X_pad ** 2, -1))\n",
    "X_norm = (X_pad.T / normalization).T\n",
    "print(\"First X sample (normalized):\", X_norm[0])\n",
    "\n",
    "Y = data[:, -1].astype(int)\n",
    "\n",
    "np.random.seed(0)\n",
    "num_data = len(Y)\n",
    "num_train = int(0.75 * num_data)\n",
    "index = np.random.permutation(range(num_data))\n",
    "feats_train = X_pad[index[:num_train]]\n",
    "Y_train = Y[index[:num_train]]\n",
    "feats_val = X_pad[index[num_train:]]\n",
    "Y_val = Y[index[num_train:]]\n",
    "\n",
    "# We need these later for plotting\n",
    "X_train = X_pad[index[:num_train]]\n",
    "X_val = X_pad[index[num_train:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Initializing the weights and optimization routing for the quantum neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_weights = strong_ent_layers_uniform(num_layers, n_qubits, seed=42)\n",
    "theta_bias = 0.0\n",
    "theta_init = (theta_weights, theta_bias)  # initial weights\n",
    "opt = AdamOptimizer(l_r)\n",
    "batch_size = 10\n",
    "var = theta_init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Full Circuit Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0: ──╭QubitStateVector(M0)──Rot(2.353, 5.974, 4.599)──╭C───────────────────────────────╭X──Rot(4.449, 0.129, 6.094)──╭C──╭X───Rot(2.714, 1.83, 3.844)────────────────────────────╭C──────╭X──┤ ⟨Z⟩ \n",
      " 1: ──├QubitStateVector(M0)──Rot(3.761, 0.98, 0.98)────╰X──╭C──Rot(5.23, 1.334, 1.142)──│─────────────────────────────│───╰C──╭X────────────────────────Rot(0.876, 1.836, 2.302)──╰X──╭C──│───┤ ⟨Z⟩ \n",
      " 2: ──╰QubitStateVector(M0)──Rot(0.365, 5.442, 3.777)──────╰X───────────────────────────╰C──Rot(1.152, 1.912, 3.297)──╰X──────╰C────────────────────────Rot(2.866, 4.933, 1.255)──────╰X──╰C──┤ ⟨Z⟩ \n",
      "M0 =\n",
      "[0.67767924 0.32715549 0.59589036 0.28041899 0.         0.\n",
      " 0.         0.        ]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "yhat = quantum_net(theta_init[0], x=X_train[0])\n",
    "print(quantum_net.draw())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    " 0: ──╭QubitStateVector(M0)──Rot(2.353, 5.974, 4.599)──╭C───────────────────────────────╭X──Rot(4.449, 0.129, 6.094)──╭C──╭X───Rot(2.714, 1.83, 3.844)────────────────────────────╭C──────╭X──┤ ⟨Z⟩ \n",
    " 1: ──├QubitStateVector(M0)──Rot(3.761, 0.98, 0.98)────╰X──╭C──Rot(5.23, 1.334, 1.142)──│─────────────────────────────│───╰C──╭X────────────────────────Rot(0.876, 1.836, 2.302)──╰X──╭C──│───┤ ⟨Z⟩ \n",
    " 2: ──╰QubitStateVector(M0)──Rot(0.365, 5.442, 3.777)──────╰X───────────────────────────╰C──Rot(1.152, 1.912, 3.297)──╰X──────╰C────────────────────────Rot(2.866, 4.933, 1.255)──────╰X──╰C──┤ ⟨Z⟩ \n",
    "M0 =\n",
    "[0.67767924 0.32715549 0.59589036 0.28041899 0.         0.\n",
    " 0.         0.        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Initiating the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:     1 | Cost: 0.4173772 | Acc train: 0.3392857 | Acc validation: 0.3157895 \n",
      "Iter:     2 | Cost: 0.4094382 | Acc train: 0.1339286 | Acc validation: 0.1578947 \n",
      "Iter:     3 | Cost: 0.4040983 | Acc train: 0.0000000 | Acc validation: 0.0000000 \n",
      "Iter:     4 | Cost: 0.3968759 | Acc train: 0.0357143 | Acc validation: 0.0526316 \n",
      "Iter:     5 | Cost: 0.3883290 | Acc train: 0.3303571 | Acc validation: 0.2894737 \n",
      "Iter:     6 | Cost: 0.3812655 | Acc train: 0.3392857 | Acc validation: 0.3157895 \n",
      "Iter:     7 | Cost: 0.3744763 | Acc train: 0.3392857 | Acc validation: 0.3157895 \n",
      "Iter:     8 | Cost: 0.3678104 | Acc train: 0.3392857 | Acc validation: 0.3157895 \n",
      "Iter:     9 | Cost: 0.3612850 | Acc train: 0.3392857 | Acc validation: 0.3157895 \n",
      "Iter:    10 | Cost: 0.3557522 | Acc train: 0.3392857 | Acc validation: 0.3157895 \n",
      "Iter:    11 | Cost: 0.3516366 | Acc train: 0.4375000 | Acc validation: 0.3947368 \n",
      "Iter:    12 | Cost: 0.3496112 | Acc train: 0.4553571 | Acc validation: 0.4210526 \n",
      "Iter:    13 | Cost: 0.3487537 | Acc train: 0.4821429 | Acc validation: 0.4210526 \n",
      "Iter:    14 | Cost: 0.3488275 | Acc train: 0.4821429 | Acc validation: 0.4473684 \n",
      "Iter:    15 | Cost: 0.3488105 | Acc train: 0.4821429 | Acc validation: 0.4473684 \n",
      "Iter:    16 | Cost: 0.3464246 | Acc train: 0.6696429 | Acc validation: 0.6052632 \n",
      "Iter:    17 | Cost: 0.3432380 | Acc train: 0.6785714 | Acc validation: 0.6315789 \n",
      "Iter:    18 | Cost: 0.3418086 | Acc train: 0.6160714 | Acc validation: 0.5526316 \n",
      "Iter:    19 | Cost: 0.3448383 | Acc train: 0.3392857 | Acc validation: 0.3157895 \n",
      "Iter:    20 | Cost: 0.3484850 | Acc train: 0.3392857 | Acc validation: 0.3157895 \n",
      "Iter:    21 | Cost: 0.3513591 | Acc train: 0.3392857 | Acc validation: 0.3157895 \n",
      "Iter:    22 | Cost: 0.3526877 | Acc train: 0.3392857 | Acc validation: 0.3157895 \n",
      "Iter:    23 | Cost: 0.3519859 | Acc train: 0.3392857 | Acc validation: 0.3157895 \n",
      "Iter:    24 | Cost: 0.3506087 | Acc train: 0.3392857 | Acc validation: 0.3157895 \n",
      "Iter:    25 | Cost: 0.3482950 | Acc train: 0.3392857 | Acc validation: 0.3157895 \n",
      "Iter:    26 | Cost: 0.3432491 | Acc train: 0.3392857 | Acc validation: 0.3157895 \n",
      "Iter:    27 | Cost: 0.3371422 | Acc train: 0.3392857 | Acc validation: 0.3157895 \n",
      "Iter:    28 | Cost: 0.3308556 | Acc train: 0.3660714 | Acc validation: 0.3684211 \n",
      "Iter:    29 | Cost: 0.3262503 | Acc train: 0.6250000 | Acc validation: 0.5000000 \n",
      "Iter:    30 | Cost: 0.3235973 | Acc train: 0.6785714 | Acc validation: 0.6315789 \n",
      "Iter:    31 | Cost: 0.3221273 | Acc train: 0.6785714 | Acc validation: 0.6315789 \n",
      "Iter:    32 | Cost: 0.3219864 | Acc train: 0.6785714 | Acc validation: 0.6315789 \n",
      "Iter:    33 | Cost: 0.3225856 | Acc train: 0.6785714 | Acc validation: 0.6315789 \n",
      "Iter:    34 | Cost: 0.3228921 | Acc train: 0.6785714 | Acc validation: 0.6315789 \n",
      "Iter:    35 | Cost: 0.3220459 | Acc train: 0.6785714 | Acc validation: 0.6315789 \n",
      "Iter:    36 | Cost: 0.3210275 | Acc train: 0.6785714 | Acc validation: 0.6315789 \n",
      "Iter:    37 | Cost: 0.3195354 | Acc train: 0.6785714 | Acc validation: 0.6315789 \n",
      "Iter:    38 | Cost: 0.3182294 | Acc train: 0.6785714 | Acc validation: 0.6315789 \n",
      "Iter:    39 | Cost: 0.3169945 | Acc train: 0.6785714 | Acc validation: 0.6315789 \n",
      "Iter:    40 | Cost: 0.3155417 | Acc train: 0.6785714 | Acc validation: 0.6315789 \n",
      "Iter:    41 | Cost: 0.3142147 | Acc train: 0.6785714 | Acc validation: 0.6315789 \n",
      "Iter:    42 | Cost: 0.3129298 | Acc train: 0.6785714 | Acc validation: 0.6315789 \n",
      "Iter:    43 | Cost: 0.3109493 | Acc train: 0.6785714 | Acc validation: 0.6315789 \n",
      "Iter:    44 | Cost: 0.3091727 | Acc train: 0.6785714 | Acc validation: 0.6315789 \n",
      "Iter:    45 | Cost: 0.3088290 | Acc train: 0.6785714 | Acc validation: 0.6315789 \n",
      "Iter:    46 | Cost: 0.3074238 | Acc train: 0.6785714 | Acc validation: 0.6315789 \n",
      "Iter:    47 | Cost: 0.3060335 | Acc train: 0.6785714 | Acc validation: 0.6315789 \n",
      "Iter:    48 | Cost: 0.3049900 | Acc train: 0.6785714 | Acc validation: 0.6315789 \n",
      "Iter:    49 | Cost: 0.3048773 | Acc train: 0.6785714 | Acc validation: 0.6315789 \n",
      "Iter:    50 | Cost: 0.3054035 | Acc train: 0.6785714 | Acc validation: 0.6315789 \n",
      "Iter:    51 | Cost: 0.3051972 | Acc train: 0.6785714 | Acc validation: 0.6315789 \n",
      "Iter:    52 | Cost: 0.3042421 | Acc train: 0.6785714 | Acc validation: 0.6315789 \n",
      "Iter:    53 | Cost: 0.3027783 | Acc train: 0.6785714 | Acc validation: 0.6315789 \n",
      "Iter:    54 | Cost: 0.3030363 | Acc train: 0.6785714 | Acc validation: 0.6315789 \n",
      "Iter:    55 | Cost: 0.3034493 | Acc train: 0.6785714 | Acc validation: 0.6315789 \n",
      "Iter:    56 | Cost: 0.3031795 | Acc train: 0.6785714 | Acc validation: 0.6315789 \n",
      "Iter:    57 | Cost: 0.3004397 | Acc train: 0.6785714 | Acc validation: 0.6315789 \n",
      "Iter:    58 | Cost: 0.2958430 | Acc train: 0.6785714 | Acc validation: 0.6315789 \n",
      "Iter:    59 | Cost: 0.2907529 | Acc train: 0.6785714 | Acc validation: 0.6315789 \n",
      "Iter:    60 | Cost: 0.2907707 | Acc train: 0.6785714 | Acc validation: 0.6315789 \n",
      "Iter:    61 | Cost: 0.2945552 | Acc train: 0.6785714 | Acc validation: 0.6315789 \n",
      "Iter:    62 | Cost: 0.2983466 | Acc train: 0.6607143 | Acc validation: 0.6052632 \n",
      "Iter:    63 | Cost: 0.2969064 | Acc train: 0.6428571 | Acc validation: 0.6315789 \n",
      "Iter:    64 | Cost: 0.2917637 | Acc train: 0.6964286 | Acc validation: 0.7105263 \n",
      "Iter:    65 | Cost: 0.2864836 | Acc train: 0.8928571 | Acc validation: 0.9210526 \n",
      "Iter:    66 | Cost: 0.2852492 | Acc train: 0.8392857 | Acc validation: 0.8947368 \n",
      "Iter:    67 | Cost: 0.2864642 | Acc train: 0.8035714 | Acc validation: 0.8157895 \n",
      "Iter:    68 | Cost: 0.2877202 | Acc train: 0.8035714 | Acc validation: 0.7894737 \n",
      "Iter:    69 | Cost: 0.2881782 | Acc train: 0.7946429 | Acc validation: 0.8421053 \n",
      "Iter:    70 | Cost: 0.2895566 | Acc train: 0.7500000 | Acc validation: 0.7894737 \n",
      "Iter:    71 | Cost: 0.2943167 | Acc train: 0.6428571 | Acc validation: 0.6578947 \n",
      "Iter:    72 | Cost: 0.2933387 | Acc train: 0.6428571 | Acc validation: 0.6842105 \n",
      "Iter:    73 | Cost: 0.2911714 | Acc train: 0.6607143 | Acc validation: 0.6842105 \n",
      "Iter:    74 | Cost: 0.2892218 | Acc train: 0.6607143 | Acc validation: 0.6842105 \n",
      "Iter:    75 | Cost: 0.2847184 | Acc train: 0.6964286 | Acc validation: 0.7105263 \n",
      "Iter:    76 | Cost: 0.2821197 | Acc train: 0.8750000 | Acc validation: 0.8947368 \n",
      "Iter:    77 | Cost: 0.2825323 | Acc train: 0.8750000 | Acc validation: 0.8947368 \n",
      "Iter:    78 | Cost: 0.2859341 | Acc train: 0.6875000 | Acc validation: 0.6578947 \n",
      "Iter:    79 | Cost: 0.2885009 | Acc train: 0.6785714 | Acc validation: 0.6315789 \n",
      "Iter:    80 | Cost: 0.2902531 | Acc train: 0.6785714 | Acc validation: 0.6315789 \n",
      "Iter:    81 | Cost: 0.2870007 | Acc train: 0.6785714 | Acc validation: 0.6315789 \n",
      "Iter:    82 | Cost: 0.2835234 | Acc train: 0.6785714 | Acc validation: 0.6315789 \n",
      "Iter:    83 | Cost: 0.2807635 | Acc train: 0.6785714 | Acc validation: 0.6315789 \n",
      "Iter:    84 | Cost: 0.2800039 | Acc train: 0.6875000 | Acc validation: 0.6578947 \n",
      "Iter:    85 | Cost: 0.2804943 | Acc train: 0.6785714 | Acc validation: 0.6315789 \n",
      "Iter:    86 | Cost: 0.2823668 | Acc train: 0.6785714 | Acc validation: 0.6315789 \n",
      "Iter:    87 | Cost: 0.2866399 | Acc train: 0.6785714 | Acc validation: 0.6315789 \n",
      "Iter:    88 | Cost: 0.2896648 | Acc train: 0.6785714 | Acc validation: 0.6315789 \n",
      "Iter:    89 | Cost: 0.2897475 | Acc train: 0.6785714 | Acc validation: 0.6315789 \n",
      "Iter:    90 | Cost: 0.2876915 | Acc train: 0.6785714 | Acc validation: 0.6315789 \n",
      "Iter:    91 | Cost: 0.2857767 | Acc train: 0.6785714 | Acc validation: 0.6315789 \n",
      "Iter:    92 | Cost: 0.2819118 | Acc train: 0.6875000 | Acc validation: 0.6578947 \n",
      "Iter:    93 | Cost: 0.2792041 | Acc train: 0.8392857 | Acc validation: 0.8684211 \n",
      "Iter:    94 | Cost: 0.2784087 | Acc train: 0.9285714 | Acc validation: 0.9473684 \n",
      "Iter:    95 | Cost: 0.2780946 | Acc train: 0.8035714 | Acc validation: 0.8421053 \n",
      "Iter:    96 | Cost: 0.2785098 | Acc train: 0.7767857 | Acc validation: 0.7894737 \n",
      "Iter:    97 | Cost: 0.2788517 | Acc train: 0.8035714 | Acc validation: 0.8421053 \n",
      "Iter:    98 | Cost: 0.2805824 | Acc train: 0.8660714 | Acc validation: 0.8947368 \n",
      "Iter:    99 | Cost: 0.2811501 | Acc train: 0.8839286 | Acc validation: 0.8947368 \n",
      "Iter:   100 | Cost: 0.2819560 | Acc train: 0.9285714 | Acc validation: 0.9473684 \n"
     ]
    }
   ],
   "source": [
    "for it in range(num_epochs):\n",
    "    # Update the weights by one optimizer step\n",
    "    batch_index = np.random.randint(0, num_train, (batch_size,))\n",
    "    feats_train_batch = feats_train[batch_index]\n",
    "    Y_train_batch = Y_train[batch_index]\n",
    "    var = opt.step(lambda v: cost(v, feats_train_batch, Y_train_batch), var)\n",
    "    predictions_train = [quantum_net(var[0], x=f) for f in feats_train]\n",
    "    predictions_val = [quantum_net(var[0], x=f) for f in feats_val]\n",
    "    pred_train_labels = np.argmax(softmax(predictions_train), axis=1)\n",
    "    pred_val_labels = np.argmax(softmax(predictions_val), axis=1)\n",
    "    # Compute accuracy on train and validation set\n",
    "    acc_train = np.mean(Y_train==pred_train_labels)\n",
    "    acc_val = np.mean(Y_val==pred_val_labels)\n",
    "    print(\n",
    "        \"Iter: {:5d} | Cost: {:0.7f} | Acc train: {:0.7f} | Acc validation: {:0.7f} \"\n",
    "        \"\".format(it + 1, cost(var, X, Y), acc_train, acc_val)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
